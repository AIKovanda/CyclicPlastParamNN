tasks: rcpl.tasks.processing.*
uses: "{CONFIGS_DIR}/dataset/maftr/d2.maftr2_4.yaml"


model:
  class: rcpl.models.gru.GRU
  kwargs:
    batchnorm: true
    in_channels: 2
    layers: 6
    hidden_size: 128
    outputs: 12
    rnn_kwargs:
      bidirectional: true
    first_last: true

takes_max_length: 681
takes_channels: ['stress', 'epsp']
do_compile: true
is_trainable: true
prediction_is_scaled: true
# chosen_checkpoint: 199


persistent_training_params:
  epochs: 10

  optim: 'AdamW'
  optim_kwargs:
    lr: 0.001
    weight_decay: 0.01

  dataloader_kwargs:
    batch_size: 256
    shuffle: true
    num_workers: 12
    drop_last: true
    pin_memory: true

  scheduler: "torch.optim.lr_scheduler.CosineAnnealingLR(opt, len(train_ldr) * epochs + 1)"
  exec_str: 'nn.utils.clip_grad_value_(model.parameters(), 0.5)'

  checkpoint_n: 4000

other_training_params:

  valid_dataloader_kwargs:
    batch_size: 256
    shuffle: false
    num_workers: 12
    drop_last: true
    pin_memory: true

  save_metrics_n: 100
  evaluate_n: 1000

  metrics:
    x_metrics_items: 32
    x_metrics:
      x_l2: 'x_l2'

    y_metrics:
      y_l2: 'y_l2'

eval_on_experiments: [ '2023-11-23', '2023-11-07' ]
