tasks: rcpl.tasks.processing.*
uses: "{CONFIGS_DIR}/dataset/maftr/d2.maftr2_4.yaml"


model:
  class: rcpl.models.fc.FC
  kwargs:
    inputs: 681
    layers: [512, 256, 128]
    outputs: 12
    in_channels: 1
    batchnorm: true

takes_max_length: 681
takes_channels: ['stress', 'epsp']
do_compile: true
is_trainable: true
prediction_is_scaled: true
# chosen_checkpoint: 199


persistent_training_params:
  epochs: 2

  optim: 'AdamW'
  optim_kwargs:
    lr: 0.001
    weight_decay: 0.01

  dataloader_kwargs:
    batch_size: 256
    shuffle: true
    num_workers: 12
    drop_last: true
    pin_memory: true

  scheduler: "torch.optim.lr_scheduler.CosineAnnealingLR(opt, len(train_ldr) * epochs + 1)"
  exec_str: 'nn.utils.clip_grad_value_(model.parameters(), 0.5)'

  checkpoint_n: 4000

other_training_params:

  valid_dataloader_kwargs:
    batch_size: 256
    shuffle: false
    num_workers: 12
    drop_last: true
    pin_memory: true

  save_metrics_n: 100
  evaluate_n: 1000

  metrics:
    x_metrics_items: 32
    x_metrics:
      x_l2: 'x_l2'

    y_metrics:
      y_l2: 'y_l2'

eval_on_experiments: [ '2023-11-23' ]

loss_func: 'MSELoss'
loss_func_kwargs:
  y_scale: 0.5
  x_scale: (1-0.5)*30
  y_crop: -1

single_score_definition: "mm['exp/2023-11-23n'] / 10 + mm['simplex']['new_x_l2']"
