tasks: rcpl.tasks.processing.*
uses: "{CONFIGS_DIR}/dataset/maftr/maftr2_4.yaml"

model:
  class: rcpl.estimation_model.gru.GRU
  kwargs:
    batchnorm: true
    in_channels: 2
    layers: 5
    hidden_size: 110
    outputs: 12
    rnn_kwargs:
      bidirectional: true
    first_last: true

takes_max_length: 681
takes_channels: ['stress', 'epsp']
do_compile: true
is_trainable: true
prediction_is_scaled: true
# chosen_checkpoint: 199


persistent_training_params:
  epochs: 2

  optim_kwargs:
    lr: 0.003252437300053346
    weight_decay: 0.01342520499395633

  dataloader_kwargs:
    batch_size: 128
    shuffle: true
    num_workers: 12
    drop_last: true
    pin_memory: true

  optim: 'AdamW'
  scheduler: "torch.optim.lr_scheduler.CosineAnnealingLR(opt, len(train_ldr) * epochs + 1)"
  exec_str: 'nn.utils.clip_grad_value_(model.parameters(), 0.5)'

  checkpoint_n: 4000

other_training_params:

  valid_dataloader_kwargs:
    batch_size: 256
    shuffle: false
    num_workers: 12
    drop_last: true
    pin_memory: true

  save_metrics_n: 100
  evaluate_n: 1000

  metrics:
    x_metrics_items: 32
    x_metrics:
      x_l2: 'x_l2'
      x_l2_median: 'x_l2_median'

    y_metrics:
      y_l2: 'y_l2'
      y_l2_median: 'y_l2_median'

eval_on_experiments: [ '2023-11-23' ]


run_name: 'maftr2_4.best/gru'
loss_func: 'MSELoss'
loss_func_kwargs:
  y_scale: 0.5
  x_scale: (1-0.5)*30
  y_crop: -1

single_score_definition: "mm['exp/2023-11-23n'] / 10 + mm['simplex']['new_x_l2']"
