tasks: rcpl.tasks.processing.*
uses: "{CONFIGS_DIR}/dataset/maf/2023-08-27.d1.2_4.maf2_4.yaml"

architecture: InceptionTime
takes_max_length: 603
takes_epsp: false
model_parameters:
  batchnorm: true
  build_config:
    - bottleneck_channels: 64
      kernel_sizes: [5, 15, 25]
      n_filters: 32
      # pooling_size: 8
      # pooling_type: max
      use_batch_norm: true
      activation: relu
    - bottleneck_channels: 64
      kernel_sizes: [5, 15, 25]
      n_filters: 32
      # pooling_size: 8
      # pooling_type: max
      use_batch_norm: true
      activation: relu
    - bottleneck_channels: 64
      kernel_sizes: [5, 15, 25]
      n_filters: 32
      # pooling_size: 8
      # pooling_type: max
      use_batch_norm: true
      # activation: sigmoid

  out_activation: none
  in_channels: 1
  outputs: 11

# training_params:
optim: 'AdamW'
optim_kwargs:
  lr: 0.001
  betas: [0.9, 0.99]
  eps: .00000001
  weight_decay: 0.01

loss_func: 'epsp'
loss_func_kwargs:
  y_ratio: 0.15
  x_grad_scale: 30.  # empirically similar

epochs: 5
scheduler: "torch.optim.lr_scheduler.CosineAnnealingLR(opt, len(train_ldr) * epochs + 1)"
exec_str: 'nn.utils.clip_grad_value_(net.parameters(), 0.5)'
save_metrics_n: 100
evaluate_n: 300
checkpoint_n: 2000

# dataloader_params:
batch_size: 128
shuffle: true
num_workers: 12
drop_last: true

valid_batch_size: 256
do_compile: true
run_dir: "standard"
run_name: "it4-4"


x_metrics:
  x_l2: 'metrics.x_l2'

y_metrics:
  y_l2: 'metrics.y_l2'
